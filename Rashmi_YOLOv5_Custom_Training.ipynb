{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rashmi-carol-dsouza/robot-dream/blob/main/Rashmi_YOLOv5_Custom_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrsaDfdVHzxt"
      },
      "source": [
        "# Custom Training with YOLOv5\n",
        "\n",
        "In this tutorial, we assemble a dataset and train a custom YOLOv5 model to recognize the objects in our dataset. To do so we will take the following steps:\n",
        "\n",
        "* Gather a dataset of images and label our dataset\n",
        "* Export our dataset to YOLOv5\n",
        "* Train YOLOv5 to recognize the objects in our dataset\n",
        "* Evaluate our YOLOv5 model's performance\n",
        "* Run test inference to view our model at work\n",
        "\n",
        "\n",
        "\n",
        "![](https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNveqeA1KXGy"
      },
      "source": [
        "# Step 1: Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTvDNSILZoN9",
        "outputId": "7cdcc108-590d-41a6-b2ce-55f4b11ca753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 14927, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 14927 (delta 7), reused 13 (delta 3), pack-reused 14908\u001b[K\n",
            "Receiving objects: 100% (14927/14927), 14.01 MiB | 31.04 MiB/s, done.\n",
            "Resolving deltas: 100% (10246/10246), done.\n",
            "/content/yolov5\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 KB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Setup complete. Using torch 1.13.0+cu116 (Tesla T4)\n"
          ]
        }
      ],
      "source": [
        "#clone YOLOv5 and \n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "%pip install -q roboflow\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP6USLgz2f0r"
      },
      "source": [
        "# Step 2: Assemble Our Dataset\n",
        "\n",
        "In order to train our custom model, we need to assemble a dataset of representative images with bounding box annotations around the objects that we want to detect. And we need our dataset to be in YOLOv5 format.\n",
        "\n",
        "In Roboflow, you can choose between two paths:\n",
        "\n",
        "* Convert an existing dataset to YOLOv5 format. Roboflow supports over [30 formats object detection formats](https://roboflow.com/formats) for conversion.\n",
        "* Upload raw images and annotate them in Roboflow with [Roboflow Annotate](https://docs.roboflow.com/annotate).\n",
        "\n",
        "# Annotate\n",
        "\n",
        "![](https://roboflow-darknet.s3.us-east-2.amazonaws.com/roboflow-annotate.gif)\n",
        "\n",
        "# Version\n",
        "\n",
        "![](https://roboflow-darknet.s3.us-east-2.amazonaws.com/robolfow-preprocessing.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2wGvjd4Z_92",
        "outputId": "e7fd8a21-d02d-47b0-d18a-3a4eadd57f53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "upload and label your dataset, and get an API KEY here: https://app.roboflow.com/?model=yolov5&ref=ultralytics\n"
          ]
        }
      ],
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(model_format=\"yolov5\", notebook=\"ultralytics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2jjT5uIHo6l5"
      },
      "outputs": [],
      "source": [
        "# set up environment\n",
        "os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwJcaoPGF4VI"
      },
      "outputs": [],
      "source": [
        "#after following the link above, recieve python code with these fields filled in\n",
        "#from roboflow import Roboflow\n",
        "#rf = Roboflow(api_key=\"YOUR API KEY HERE\")\n",
        "#project = rf.workspace().project(\"YOUR PROJECT\")\n",
        "#dataset = project.version(\"YOUR VERSION\").download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVAKA6Cs7XCN",
        "outputId": "f501c5d2-2abc-4b5e-a7e0-abcd3b7c7d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in /content/datasets/Final-Dataset-1 to yolov5pytorch: 100% [904258785 / 904258785] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to /content/datasets/Final-Dataset-1 in yolov5pytorch:: 100%|██████████| 4723/4723 [00:04<00:00, 1028.04it/s]\n"
          ]
        }
      ],
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"QsgLH4IMByL0vL5GL79W\")\n",
        "project = rf.workspace(\"balloon-emler\").project(\"final-dataset-mhybw\")\n",
        "dataset = project.version(1).download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7yAi9hd-T4B"
      },
      "source": [
        "# Step 3: Train Our Custom YOLOv5 model\n",
        "\n",
        "Here, we are able to pass a number of arguments:\n",
        "- **img:** define input image size\n",
        "- **batch:** determine batch size\n",
        "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
        "- **data:** Our dataset locaiton is saved in the `dataset.location`\n",
        "- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n",
        "- **cache:** cache images for faster training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaFNnxLJbq4J"
      },
      "outputs": [],
      "source": [
        "#!python train.py --img 400 --batch 16 --epochs 150 --data /content/datasets/Final-Dataset-1/data.yaml --weights yolov5s.pt --cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJf7s-Y0ffXr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcIRLQOlA14A"
      },
      "source": [
        "# Evaluate Custom YOLOv5 Detector Performance\n",
        "Training losses and performance metrics are saved to Tensorboard and also to a logfile.\n",
        "\n",
        "If you are new to these metrics, the one you want to focus on is `mAP_0.5` - learn more about mean average precision [here](https://blog.roboflow.com/mean-average-precision/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69y1uSxvfm9c",
        "outputId": "9b953a5f-a999-49fd-dc3a-c9e9937b289e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5l.pt, cfg=, data=/content/datasets/Final-Dataset-1/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=800, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-56-gc0ca1d2 Python-3.8.16 torch-1.13.0+cu116 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 125MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l.pt to yolov5l.pt...\n",
            "100% 89.3M/89.3M [00:00<00:00, 106MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
            "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
            "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
            "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
            "  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
            "  9                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
            " 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
            " 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
            " 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
            " 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  3   9971712  models.common.C3                        [1024, 1024, 3, False]        \n",
            " 24      [17, 20, 23]  1     32310  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
            "Model summary: 368 layers, 46138294 parameters, 46138294 gradients, 108.2 GFLOPs\n",
            "\n",
            "Transferred 607/613 items from yolov5l.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 101 weight(decay=0.0), 104 weight(decay=0.0005), 104 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/Final-Dataset-1/train/labels... 1999 images, 913 backgrounds, 0 corrupt: 100% 1999/1999 [00:00<00:00, 2110.98it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/Final-Dataset-1/train/labels.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.6GB ram): 100% 1999/1999 [00:58<00:00, 34.10it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/Final-Dataset-1/valid/labels... 358 images, 307 backgrounds, 0 corrupt: 100% 358/358 [00:00<00:00, 793.14it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/Final-Dataset-1/valid/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.4GB ram): 100% 358/358 [01:00<00:00,  5.90it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.58 anchors/target, 0.999 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "Image sizes 800 train, 800 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/49      14.3G    0.07912    0.05373          0         91        800: 100% 125/125 [02:20<00:00,  1.12s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:08<00:00,  1.45it/s]\n",
            "                   all        358        568      0.363      0.276      0.211     0.0939\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/49      13.6G    0.06007    0.03701          0         88        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:06<00:00,  1.97it/s]\n",
            "                   all        358        568      0.325      0.559      0.394      0.147\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/49      13.6G    0.05312    0.03506          0         89        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.01it/s]\n",
            "                   all        358        568      0.223      0.285      0.144     0.0767\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/49      13.6G    0.04646    0.03487          0        124        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:06<00:00,  1.99it/s]\n",
            "                   all        358        568      0.476      0.331      0.248      0.176\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/49      13.6G    0.04261    0.03253          0        108        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:06<00:00,  1.99it/s]\n",
            "                   all        358        568      0.501      0.371       0.33      0.232\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/49      13.6G    0.03952    0.03133          0         85        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.00it/s]\n",
            "                   all        358        568      0.567      0.364      0.344      0.248\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/49      13.6G    0.03744      0.029          0         70        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.01it/s]\n",
            "                   all        358        568      0.588      0.523      0.581        0.4\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/49      13.6G    0.03657    0.03094          0         44        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:06<00:00,  1.99it/s]\n",
            "                   all        358        568      0.577      0.405      0.439      0.327\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/49      13.6G    0.03547    0.02917          0         70        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:06<00:00,  1.99it/s]\n",
            "                   all        358        568       0.51      0.366      0.369      0.266\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/49      13.6G    0.03462    0.02867          0         56        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.01it/s]\n",
            "                   all        358        568      0.564      0.421       0.48      0.355\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/49      13.6G    0.03462    0.02776          0         98        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.01it/s]\n",
            "                   all        358        568      0.483      0.438      0.448      0.331\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      11/49      13.6G    0.03133    0.02539          0         47        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.00it/s]\n",
            "                   all        358        568      0.622      0.386      0.375      0.284\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      12/49      13.6G    0.03268    0.02723          0        102        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.01it/s]\n",
            "                   all        358        568      0.488      0.553      0.512      0.375\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      13/49      13.6G    0.03224    0.02544          0        127        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.02it/s]\n",
            "                   all        358        568      0.627       0.41       0.39      0.295\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      14/49      13.6G    0.03116      0.026          0        117        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.02it/s]\n",
            "                   all        358        568      0.652      0.373      0.414      0.322\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      15/49      13.6G    0.03066    0.02466          0         95        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.03it/s]\n",
            "                   all        358        568       0.61      0.377      0.372      0.289\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      16/49      13.6G    0.03059    0.02299          0         88        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.02it/s]\n",
            "                   all        358        568       0.76      0.406      0.399      0.311\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      17/49      13.6G    0.02999     0.0233          0         91        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.03it/s]\n",
            "                   all        358        568      0.667        0.4      0.413       0.32\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      18/49      13.6G    0.02937    0.02237          0         89        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.02it/s]\n",
            "                   all        358        568      0.697      0.402      0.417      0.327\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      19/49      13.6G    0.02838    0.02301          0        104        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.01it/s]\n",
            "                   all        358        568      0.708      0.665      0.691      0.519\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      20/49      13.6G    0.02926    0.02366          0         97        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.03it/s]\n",
            "                   all        358        568      0.558      0.484      0.537      0.404\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      21/49      13.6G    0.02868    0.02202          0        138        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.03it/s]\n",
            "                   all        358        568      0.602       0.63      0.614      0.468\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      22/49      13.6G    0.02785    0.02128          0         81        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.01it/s]\n",
            "                   all        358        568      0.796      0.799      0.779      0.593\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      23/49      13.6G    0.02767    0.02104          0        111        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.02it/s]\n",
            "                   all        358        568      0.599      0.442      0.472       0.37\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      24/49      13.6G    0.02767    0.02069          0        103        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.03it/s]\n",
            "                   all        358        568      0.595      0.473      0.525      0.405\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      25/49      13.6G    0.02639    0.02115          0         94        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:06<00:00,  1.95it/s]\n",
            "                   all        358        568      0.555      0.664      0.604      0.463\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      26/49      13.6G    0.02702    0.02061          0         59        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.04it/s]\n",
            "                   all        358        568      0.709      0.423      0.434      0.344\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      27/49      13.6G    0.02651    0.02047          0         85        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.03it/s]\n",
            "                   all        358        568      0.806       0.79       0.79      0.611\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      28/49      13.6G    0.02554    0.01983          0         57        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.03it/s]\n",
            "                   all        358        568      0.769      0.727      0.716      0.554\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      29/49      13.6G      0.026    0.01959          0        176        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.04it/s]\n",
            "                   all        358        568      0.524      0.512      0.498      0.383\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      30/49      13.6G     0.0258    0.01975          0         48        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.01it/s]\n",
            "                   all        358        568      0.599      0.606      0.618      0.475\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      31/49      13.6G    0.02575    0.01962          0         55        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.02it/s]\n",
            "                   all        358        568      0.646      0.498      0.549      0.425\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      32/49      13.6G    0.02478    0.01823          0         39        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.03it/s]\n",
            "                   all        358        568      0.811      0.393      0.463      0.365\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      33/49      13.6G    0.02503    0.01897          0         70        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.03it/s]\n",
            "                   all        358        568      0.693      0.467      0.517      0.405\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      34/49      13.6G    0.02404    0.01868          0         55        800: 100% 125/125 [02:17<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.02it/s]\n",
            "                   all        358        568      0.765      0.681      0.715      0.556\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      35/49      13.6G    0.02456    0.01837          0        175        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.04it/s]\n",
            "                   all        358        568      0.579      0.384      0.384      0.308\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      36/49      13.6G    0.02321    0.01724          0        113        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.04it/s]\n",
            "                   all        358        568      0.633      0.549      0.594      0.463\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      37/49      13.6G    0.02421    0.01741          0        111        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.04it/s]\n",
            "                   all        358        568      0.724      0.396      0.438      0.352\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      38/49      13.6G    0.02371    0.01774          0        143        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.04it/s]\n",
            "                   all        358        568      0.474      0.532      0.469      0.375\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      39/49      13.6G    0.02384    0.01827          0         72        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.04it/s]\n",
            "                   all        358        568      0.694        0.4      0.415      0.333\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      40/49      13.6G    0.02266    0.01675          0         91        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:06<00:00,  1.96it/s]\n",
            "                   all        358        568      0.681      0.409      0.418      0.334\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      41/49      13.6G    0.02286    0.01743          0         33        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.04it/s]\n",
            "                   all        358        568      0.749      0.394      0.416      0.335\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      42/49      13.6G    0.02238     0.0159          0        115        800: 100% 125/125 [02:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.05it/s]\n",
            "                   all        358        568      0.725      0.387      0.422      0.341\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      43/49      13.6G    0.02304    0.01714          0        130        800: 100% 125/125 [02:17<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.06it/s]\n",
            "                   all        358        568       0.75      0.386      0.428      0.341\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      44/49      13.6G    0.02226    0.01689          0         53        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.05it/s]\n",
            "                   all        358        568      0.638      0.437      0.431      0.348\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      45/49      13.6G    0.02203    0.01714          0        129        800: 100% 125/125 [02:17<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.05it/s]\n",
            "                   all        358        568      0.708        0.4      0.422      0.338\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      46/49      13.6G    0.02186    0.01695          0        107        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.05it/s]\n",
            "                   all        358        568      0.676      0.412      0.424      0.341\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      47/49      13.6G    0.02162    0.01481          0         82        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.03it/s]\n",
            "                   all        358        568      0.657      0.417      0.411      0.331\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      48/49      13.6G    0.02091    0.01583          0         83        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.06it/s]\n",
            "                   all        358        568      0.719      0.403      0.423      0.339\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      49/49      13.6G    0.02113    0.01523          0         26        800: 100% 125/125 [02:18<00:00,  1.11s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:05<00:00,  2.05it/s]\n",
            "                   all        358        568      0.676      0.421      0.428      0.343\n",
            "\n",
            "50 epochs completed in 2.026 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 92.8MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 92.8MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 267 layers, 46108278 parameters, 0 gradients, 107.6 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:08<00:00,  1.44it/s]\n",
            "                   all        358        568      0.807      0.792       0.79      0.611\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python train.py --img 800 --batch 16  --epochs 50 --data /content/datasets/Final-Dataset-1/data.yaml --weights yolov5l.pt --cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "1jS9_BxdBBHL",
        "outputId": "b691e0c3-0e95-425a-e41c-57afbff22a56"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Start tensorboard\n",
        "# Launch after you have started training\n",
        "# logs save in the folder \"runs\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtmS7_TXFsT3"
      },
      "source": [
        "#Run Inference  With Trained Weights\n",
        "Run inference with a pretrained checkpoint on contents of `test/images` folder downloaded from Roboflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWjjiBcic3Vz"
      },
      "outputs": [],
      "source": [
        "!python detect.py --weights runs/train/exp/weights/best.pt --img 800 --conf 0.1 --source /content/datasets/Final-Dataset-1/valid/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZbUn4_b9GCKO"
      },
      "outputs": [],
      "source": [
        "#display inference on ALL test images\n",
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for imageName in glob.glob('/content/yolov5/runs/detect/exp3/*.jpg'): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8oQHXYmHKq3M",
        "outputId": "2402ceab-346d-42f6-f862-2e757c8da936"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-da0ec9f46589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#model = torch.hub.load('.', 'custom', path='yolov5/runs/train/exp8/weights/best.pt', source='local')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yolov5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'custom'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'yolov5/runs/train/exp4/weights/best.pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce_reload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'local'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m                                            verbose=verbose, skip_validation=skip_validation)\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0mhubconf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhubconf_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODULE_HUBCONF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m     \u001b[0mhub_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_import_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODULE_HUBCONF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhubconf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_entry_from_hubconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36m_import_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_from_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yolov5/hubconf.py'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "#model = torch.hub.load('.', 'custom', path='yolov5/runs/train/exp8/weights/best.pt', source='local') \n",
        "model = torch.hub.load('yolov5','custom', path='yolov5/runs/train/exp4/weights/best.pt',force_reload=True,source='local')\n",
        "\n",
        "# Image\n",
        "img = \"lucas-hoang-rquNBlHgSWg-unsplash.jpg\"\n",
        "# Inference\n",
        "results = model(img)\n",
        "# Results, change the flowing to: results.show()\n",
        "results.show()  # or .show(), .save(), .crop(), .pandas(), etc\n",
        "results.save(save_dir='results') \n",
        "print(results.pandas().xyxy[0])\n",
        "results.pandas().xyxy[0].value_counts('name')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LQG8jRmL0Yq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "\n",
        "# Images\n",
        "dir = 'https://github.com/ultralytics/yolov5/raw/master/data/images/'\n",
        "imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')]  # batch of images\n",
        "\n",
        "# Inference\n",
        "results = model(imgs)\n",
        "results.print()  # or .show(), .save()\n",
        "print(results.pandas().xyxy[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGh4j86GMXd8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # yolov5n - yolov5x6 official model\n",
        "#                                            'custom', 'path/to/best.pt')  # custom model\n",
        "\n",
        "# Images\n",
        "im = 'https://ultralytics.com/images/zidane.jpg'  # or file, Path, URL, PIL, OpenCV, numpy, list\n",
        "\n",
        "# Inference\n",
        "results = model(im)\n",
        "\n",
        "# Results\n",
        "results.print()  # or .show(), .save(), .crop(), .pandas(), etc.\n",
        "results.xyxy[0]  # im predictions (tensor)\n",
        "\n",
        "results.pandas().xyxy[0]  # im predictions (pandas)\n",
        "#      xmin    ymin    xmax   ymax  confidence  class    name\n",
        "# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\n",
        "# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\n",
        "# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie\n",
        "\n",
        "results.pandas().xyxy[0].value_counts('name')  # class counts (pandas)\n",
        "# person    2\n",
        "# tie       1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8dHcni6CJYt"
      },
      "source": [
        "# Conclusion and Next Steps\n",
        "\n",
        "Congratulations! You've trained a custom YOLOv5 model to recognize your custom objects.\n",
        "\n",
        "To improve you model's performance, we recommend first interating on your datasets coverage and quality. See this guide for [model performance improvement](https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results).\n",
        "\n",
        "To deploy your model to an application, see this guide on [exporting your model to deployment destinations](https://github.com/ultralytics/yolov5/issues/251).\n",
        "\n",
        "Once your model is in production, you will want to continually iterate and improve on your dataset and model via [active learning](https://blog.roboflow.com/what-is-active-learning/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7iiObB2WCMh6",
        "outputId": "6da91242-286a-4af6-bae4-63e95554a860"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_19228678-b155-4dc8-9f7f-74f7662c5d1d\", \"best.pt\", 92808021)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#export your model's weights for future use\n",
        "from google.colab import files\n",
        "files.download('./runs/train/exp/weights/best.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNn-obvOGITm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}